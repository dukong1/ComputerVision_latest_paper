{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbcde8e",
   "metadata": {},
   "source": [
    "# Ch01. Context Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec47663",
   "metadata": {},
   "source": [
    "### 단순 이미지 object detection 가 아니라 사진 전반적인 정보를 바탕으로 Scene Understanding을 하는 방법 = local feature가 같더라도 global context에 따라 scene의 의미가 달라질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e203a",
   "metadata": {},
   "source": [
    "<img src= \"https://cobslab.com/wp-content/uploads/2022/06/KakaoTalk_20220610_192243837_11-980x463.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea675a",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53278832",
   "metadata": {},
   "source": [
    "<img src =\"https://anyline.com/app/uploads/2022/01/figure_1.jpg.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb602fe",
   "metadata": {},
   "source": [
    "<img src = \"https://i.imgur.com/Rk5wkBQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878518c",
   "metadata": {},
   "source": [
    "# Transformer 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6ea6a",
   "metadata": {},
   "source": [
    "## SimpleAttention\n",
    "\n",
    "### - Attention Block\n",
    "### - MultiHeadAttention\n",
    "### - Encoder Layer\n",
    "### - Pytorch Official Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7edc36",
   "metadata": {},
   "source": [
    "# 1. Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f65c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1303da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "#device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef433938",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/dukong1/ComputerVision_latest_paper/blob/main/Ch01.%20Context_Understanding/20230412_112319.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e51db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(torch.randn(8,3)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e8b5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5320,  0.4759, -0.2298],\n",
      "        [-0.5578, -0.1919, -1.0992],\n",
      "        [ 0.0023,  0.8613, -1.6658],\n",
      "        [-0.4668, -0.6010,  0.3714],\n",
      "        [-0.0131,  0.4123, -1.3400],\n",
      "        [-0.4208,  0.6782, -1.5569],\n",
      "        [-0.1636, -1.5453, -1.5914],\n",
      "        [ 0.2377, -0.6397,  1.0430]])\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4752ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "W_K = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)\n",
    "W_V = torch.nn.Parameter(torch.Tensor(torch.randn(3,2))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323fef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9201,  0.2973],\n",
      "        [ 0.9331, -1.8127],\n",
      "        [-1.3428,  0.4633]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9912, -0.8361],\n",
      "        [-0.3777,  0.3517],\n",
      "        [-0.9956, -0.4792]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1911, -1.3999],\n",
      "        [ 0.8213,  0.7084],\n",
      "        [ 1.7573,  0.6895]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W_Q)\n",
    "print(W_K)\n",
    "print(W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93eb5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.matmul(X,W_Q) \n",
    "K = torch.mm(X,W_K)\n",
    "V = X@W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040661cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2422, -1.1273],\n",
      "        [ 1.8102, -0.3273],\n",
      "        [ 3.0384, -2.3324],\n",
      "        [-0.6300,  1.1227],\n",
      "        [ 2.1961, -1.3721],\n",
      "        [ 3.1105, -2.0757],\n",
      "        [ 0.8456,  2.0153],\n",
      "        [-2.2161,  1.7135]], grad_fn=<MmBackward0>)\n",
      "tensor([[-4.7822e-01,  7.2236e-01],\n",
      "        [ 6.1387e-01,  9.2565e-01],\n",
      "        [ 1.3355e+00,  1.0992e+00],\n",
      "        [-6.0549e-01,  9.4737e-04],\n",
      "        [ 1.1654e+00,  7.9807e-01],\n",
      "        [ 8.7680e-01,  1.3364e+00],\n",
      "        [ 2.0058e+00,  3.5585e-01],\n",
      "        [-5.6115e-01, -9.2353e-01]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 8.8615e-02,  9.2343e-01],\n",
      "        [-1.9825e+00, -1.1281e-01],\n",
      "        [-2.2203e+00, -5.4159e-01],\n",
      "        [ 2.4837e-01,  4.8382e-01],\n",
      "        [-2.0137e+00, -6.1348e-01],\n",
      "        [-2.0985e+00, -3.9164e-03],\n",
      "        [-4.0344e+00, -1.9628e+00],\n",
      "        [ 1.2620e+00, -6.6873e-02]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Q) #(8,2)\n",
    "print(K) #(8,2)\n",
    "print(V) #(8,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0a4f9",
   "metadata": {},
   "source": [
    "### Query, Key 매칭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b609a",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/dukong1/ComputerVision_latest_paper/blob/main/Ch01.%20Context_Understanding/20230412_113633.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c344b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.shape torch.Size([8, 2])\n",
      "K.T.shape torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "attention_score = Q.matmul(K.T)\n",
    "print(\"K.shape\", K.shape)\n",
    "print(\"K.T.shape\", K.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b43e842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4084, -0.2809,  0.4198, -0.7532,  0.5480, -0.4173,  2.0905,  0.3440],\n",
       "        [-1.1021,  0.8083,  2.0577, -1.0964,  1.8485,  1.1498,  3.5145, -0.7135],\n",
       "        [-3.1379, -0.2939,  1.4938, -1.8419,  1.6795, -0.4529,  5.2644,  0.4491],\n",
       "        [ 1.1123,  0.6525,  0.3927,  0.3825,  0.1618,  0.9480, -0.8642, -0.6833],\n",
       "        [-2.0414,  0.0781,  1.4247, -1.3310,  1.4644,  0.0920,  3.9168,  0.0348],\n",
       "        [-2.9870, -0.0120,  1.8724, -1.8854,  1.9685, -0.0466,  5.5004,  0.1715],\n",
       "        [ 1.0514,  2.3845,  3.3444, -0.5101,  2.5938,  3.4345,  2.4132, -2.3356],\n",
       "        [ 2.2976,  0.2257, -1.0761,  1.3435, -1.2153,  0.3467, -3.8354, -0.3389]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score #(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e56cf243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = K.shape[1]\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee883ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_score = attention_score / (d_k**0.5)\n",
    "# attention_score = attention_score / (math.sqrt(d_k)) 위와 동일 수식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4089a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_score = torch.softmax(attention_score, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "071cb94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0336, 0.0745, 0.1223, 0.0534, 0.1339, 0.0677, 0.3986, 0.1159],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d08e18a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b66a53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd6935",
   "metadata": {},
   "source": [
    "#### 연산을 줄이기 위해 Q에 먼저 나눠서 계산함\n",
    "\n",
    "#### d_k = K.shape[1]\n",
    "#### print(d_k)\n",
    "\n",
    "#### attention_score = (Q / (d_k**0.5)).matmul(K.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933356e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = attention_score@V #(8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6488dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7414843b",
   "metadata": {},
   "source": [
    "# 2. MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf0b06",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/dukong1/ComputerVision_latest_paper/blob/main/Ch01.%20Context_Understanding/20230412_122155.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a236b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
